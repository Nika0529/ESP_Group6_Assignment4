LMMsetup <- function(form, dat, ref) {
  # 构建固定效应矩阵 X 和响应变量 y
  X <- model.matrix(form, data = dat)
  y <- model.response(model.frame(form, data = dat))
  
  # 初始化随机效应矩阵 Z
  Z <- NULL
  # 根据 ref 构建 Z 矩阵
  if (length(ref) > 0) {
    for (vector in ref) {
      if (length(vector) > 1) {
        # 构建交互项块 Z_block
        Z_block <- model.matrix(as.formula(paste("~", paste(vector, collapse = ":"), "- 1")), data = dat)
      } else {
        # 构建单变量块 Z_block
        Z_block <- model.matrix(as.formula(paste("~", vector, "- 1")), data = dat)
      }
      # 将 Z_block 添加到 Z 矩阵中
      Z <- cbind(Z, Z_block)
    }
  }
  
  # 返回包含 X、Z 和 y 的列表
  return(list(X = X, Z = Z, y = y))
}

LMMprof <- function(theta, X, Z, y) {
  n <- nrow(Z)
  p <- ncol(Z)
  
  # Step 1: 提取方差参数
  sigma2 <- exp(theta[1])^2  # 残差的方差 sigma^2
  psi <- diag(exp(theta[-1])^2)  # 随机效应方差矩阵 Psi_theta
  
  # Step 2: 对 Z 进行 QR 分解，获取 QR 对象
  QR_Z <- qr(Z)
  R <- qr.R(QR_Z)  # 获取 R 矩阵
  
  # Step 3: 构建 A = R Psi_theta R^T + I_p sigma^2
  A <- R %*% psi %*% t(R) + diag(sigma2, p)

  # Step 4: 使用 Cholesky 分解求解线性系统，构建 W y 和 W X
  # 将 y 和 X 投影到 Q 的空间
  QTy <- qr.qty(QR_Z, y)  # 使用 qr.qty 计算 Q^T y
  QTX <- qr.qty(QR_Z, X)  # 使用 qr.qty 计算 Q^T X
  
  # 对 A 进行 Cholesky 分解(chol输出的函数默认是上三角函数)
  L_A <- chol(A)  # A = t(L_A) %*% L_A
  
  # 计算 W y 的分块部分
  W_y1 <- backsolve(L_A, forwardsolve(t(L_A), QTy[1:p]))  # 求解 A y_1
  W_y2 <- QTy[(p + 1):length(QTy)] / sigma2             # 后 n-p 维度的缩放
  W_y1y2 <- c(W_y1, W_y2)  # 合并分块结果

  # 计算 W X 的分块部分
  W_X1 <- apply(X_proj[1:p, , drop = FALSE], 2, function(col) {
    backsolve(L_A, forwardsolve(t(L_A), col))
  })
  W_X2 <- QTX[(p + 1):nrow(QTX), , drop = FALSE] / sigma2  # 后 n-p 维度的缩放
  W_X1X2 <- rbind(W_X1, W_X2)  # 合并分块结果

  # 使用 qr.qy 将投影结果转换回原始空间
  W_y <- qr.qy(QR_Z, W_y1y2)
  W_X <- qr.qy(QR_Z, W_X1X2)
  
  # Step 5: 计算 XtWX 和 XtWy
  XTWX <- t(X) %*% W_X
  XTWy <- t(X) %*% W_y
  
  # Step 6: 使用 Cholesky 分解计算 beta_hat
  L_XtWX <- chol(XtWX)
  beta_hat <- backsolve(L_XtWX, forwardsolve(t(L_XtWX), XtWy))

  # Step 7: 计算负对6数似然值
  # 先算(y-X%*%beta)^T(...)(y-X%*%beta)
  res <- y - X %*% beta_hat
  QTres <- qr.qty(QR_Z, res)
  W_r1 <- backsolve(L_A, forwardsolve(t(L_A), QTres[1:p]))
  W_r2 <- QTres[(p+1):n] / sigma2
  W_r1r2 <- c(W_r1, W_r2)
  W_r <- qr.qy(QR_Z, W_r1r2)
  part1 <- t(res) %*% W_r
  
  # 再算log|...| + (n-p)log(sigma2)
  det1 <- 2 * sum(log(diag(L_A)))
  det2 <- (n-p) * log(sigma2)
  part2 <- det1 + det2
  
  # 计算对数似然
  log_likelihood <- -0.5 * (part1 + part2)
  neg_log_likelihood<- -log_likelihood

   # Step 8: 将 beta_hat 作为属性添加到 neg_log_likelihood
  attr(neg_log_likelihood, "beta_hat") <- beta_hat
  
  # 返回带有属性的 neg_log_likelihood
  return(neg_log_likelihood)
}

lmm <- function(form, dat, ref = list()) {
  # 使用 LMMsetup 函数来构建 X、Z 和 y
  setup <- LMMsetup(form, dat, ref)
  X <- as.matrix(setup$X)  # 确保 X 是数值矩阵
  Z <- as.matrix(setup$Z)  # 确保 Z 是数值矩阵
  y <- setup$y
  
  # 初始化 theta
  theta_init <- rep(1, p+1)
  
  # 优化 LMMprof 以找到 theta 的最大似然估计
  opt_result <- optim(
    theta_init, 
    function(theta) as.numeric(LMMprof(theta, X, Z, y)),
  )
  
  # 提取优化结果中的负对数似然值和 theta_hat
  theta_hat <- opt_result$par
  
  # 使用最终的 theta_hat 调用 LMMprof 来获得 neg_log_likelihood 和 beta_hat
  final_result <- LMMprof(theta_hat, X, Z, y)
  neg_log_likelihood <- as.numeric(final_result)
  beta_hat <- attr(final_result, "beta_hat")
  
  # 返回结果列表
  return(list(
    neg_log_likelihood = neg_log_likelihood,
    theta_hat = theta_hat,
    beta_hat = beta_hat
  ))
}

# Test
lmm (score ~ Machine,Machines,list("Worker",c("Worker","Machine")))